{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da7ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy import Spider, Request\n",
    "from json import dump\n",
    "from os import makedirs, path\n",
    "from datetime import datetime\n",
    "\n",
    "SCRAPE_OUTPUT_DIR = \"./Acts\"\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "def windows_file_name(title):\n",
    "    \"\"\"\n",
    "    Appropriately process characters not accepted by Windows file system\n",
    "    :param title:   Title of the decision\n",
    "    :return:        Processed title\n",
    "    \"\"\"\n",
    "    # Remove \\r and \\n\n",
    "    title = re.sub(r'[\\r\\n]', '', title)\n",
    "    # Remove characters not accepted by Windows file system\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]', \"\", title)\n",
    "\n",
    "\n",
    "def save_output(data):\n",
    "    date = datetime.strptime(data[\"date\"], \"%Y-%m-%d\")\n",
    "    year = date.year\n",
    "    month = datetime.strftime(date, \"%B\")\n",
    "    output_dir = f\"{SCRAPE_OUTPUT_DIR}/{year}/{month}\"\n",
    "\n",
    "    if not path.isdir(output_dir):\n",
    "        makedirs(output_dir)\n",
    "\n",
    "    # Set DB name and identifier\n",
    "    data[\"identifier\"] = data[\"title\"]\n",
    "    data[\"database\"] = \"acts\"\n",
    "\n",
    "    file_id = windows_file_name(data[\"title\"])\n",
    "    file_name = f\"{output_dir}/{file_id}.json\"\n",
    "    with open(file_name, \"w\") as fl:\n",
    "        # Indent for readability\n",
    "        dump(data, fl, indent=0)\n",
    "\n",
    "\n",
    "def is_existing(data):\n",
    "    date = datetime.strptime(data[\"date\"], \"%Y-%m-%d\")\n",
    "    year = date.year\n",
    "    month = datetime.strftime(date, \"%B\")\n",
    "    output_dir = f\"{SCRAPE_OUTPUT_DIR}/{year}/{month}\"\n",
    "\n",
    "    file_id = windows_file_name(data[\"title\"])\n",
    "    file_name = f\"{output_dir}/{file_id}.json\"\n",
    "\n",
    "    return path.isfile(file_name)\n",
    "\n",
    "\n",
    "class ScJurisprudenceSpider(Spider):\n",
    "    name = \"sc-jurisprudence\"\n",
    "    allowed_domains = [\"elibrary.judiciary.gov.ph\"]\n",
    "    start_urls = [\"https://elibrary.judiciary.gov.ph/thebookshelf/28\"]\n",
    "\n",
    "    custom_settings = {\n",
    "        # Set up fake user agent to avoid getting blocked\n",
    "        \"USER_AGENT\": \"Mozilla/5.0\",\n",
    "        \"FEED_EXPORT_ENCODING\": \"utf-8\",\n",
    "        \"FEED_FORMAT\": \"json\",\n",
    "        # Disable robots.txt\n",
    "        \"ROBOTSTXT_OBEY\": False,\n",
    "        # Do not retry failed requests\n",
    "        \"RETRY_TIMES\": 0\n",
    "    }\n",
    "\n",
    "    # --- CONFIGS --- #\n",
    "    year = datetime.now().year - 1\n",
    "    month = 12\n",
    "\n",
    "    def __init__(self, month=None, year=None, **kwargs):\n",
    "        \"\"\"\n",
    "        This spider crawls the Supreme Court Jurisprudence website.\n",
    "\n",
    "        :param month:   Month to crawl\n",
    "        :param year:    Year to crawl\n",
    "        :param kwargs:  Other arguments\n",
    "        \"\"\"\n",
    "        if month is not None:\n",
    "            self.month = months[int(month) - 1]\n",
    "        else:\n",
    "            # Default to all months\n",
    "            self.month = None\n",
    "\n",
    "        if year is not None:\n",
    "            self.year = int(year)\n",
    "        else:\n",
    "            # Default to current year\n",
    "            self.year = datetime.now().year\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def parse(self, response):\n",
    "        soup = make_soup(response)\n",
    "        links = get_links(soup, self.month, self.year)\n",
    "\n",
    "        # Visit each link and scrape the page containing the decisions\n",
    "        for link in links:\n",
    "            yield Request(link, callback=self.parse_decision_list)\n",
    "\n",
    "    def parse_decision_list(self, response):\n",
    "        soup = make_soup(response)\n",
    "        decisions = get_decisions_metadata(soup)\n",
    "        logging.debug(f\"Processing {len(decisions)} decisions\")\n",
    "\n",
    "        # Visit each decision and scrape the decision page\n",
    "        for decision in decisions:\n",
    "            if not is_existing(decision):\n",
    "                yield Request(decision[\"url\"], callback=self.parse_decision, cb_kwargs={\"decision\": decision})\n",
    "\n",
    "    def parse_decision(self, response, decision):\n",
    "        soup = make_soup(response)\n",
    "        container = soup.find(\"div\", {\"class\": \"single_content\"})\n",
    "\n",
    "        # Get division\n",
    "        # decision[\"division\"] = container.find(\"h2\").text.strip()\n",
    "\n",
    "        # Get type\n",
    "        h3 = container.find(\"h3\")\n",
    "        h3 = str(h3)\n",
    "        # Remove \\r and \\n\n",
    "        h3 = re.sub(r'[\\r\\n]', '', h3)\n",
    "        # Split by break tag\n",
    "        h3 = h3.split('<br/>')\n",
    "        # Remove remaining tags\n",
    "        h3 = [re.sub(r'<.*?>', '', x) for x in h3]\n",
    "        # Remove empty strings and strip\n",
    "        h3 = [x.strip() for x in h3 if x.strip() != '']\n",
    "        # Assign type as the last element and remove spaces in between\n",
    "        decision[\"type\"] = h3[-1].replace(\" \", \"\")\n",
    "\n",
    "        # Get presiding justice. Find the first p tag\n",
    "        presiding_justice = container.find(\"p\")\n",
    "        # Remove the colon if present\n",
    "        # decision[\"presiding_justice\"] = presiding_justice.text.replace(':', '').strip()\n",
    "\n",
    "        # Get whole text, which is all the succeeding siblings of the presiding justice\n",
    "        whole_text = presiding_justice.find_next_siblings()\n",
    "        # Remove empty strings and strip\n",
    "        whole_text = [str(x).strip() for x in whole_text if str(x).strip() != '']\n",
    "        # Join the list\n",
    "        whole_text = ' '.join(whole_text)\n",
    "        # Remove \\r and \\n\n",
    "        whole_text = re.sub(r'[\\r\\n]', ' ', whole_text)\n",
    "        # Remove tags\n",
    "        whole_text = re.sub(r'<.*?>', ' ', whole_text)\n",
    "        # Remove multiple spaces\n",
    "        whole_text = re.sub(r'\\s+', ' ', whole_text)\n",
    "        # Remove leading and trailing spaces\n",
    "        decision[\"text\"] = whole_text.strip()\n",
    "\n",
    "        # Save output\n",
    "        save_output(decision)\n",
    "\n",
    "\n",
    "def make_soup(response):\n",
    "    \"\"\"\n",
    "    Make a BeautifulSoup object from the response.\n",
    "    :param response:    Response object\n",
    "    :return:            BeautifulSoup object\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(response.body, \"lxml\")\n",
    "\n",
    "\n",
    "def get_links(soup, month=None, year=None):\n",
    "    \"\"\"\n",
    "    Get links to the decisions for the specified month and year.\n",
    "    :param soup:    BeautifulSoup object\n",
    "    :param month:   Month to crawl (optional, defaults to all months)\n",
    "    :param year:    Year to crawl (optional, defaults to current year). 0 for all years.\n",
    "    :return:        List of links\n",
    "    \"\"\"\n",
    "    # Set up default values\n",
    "    time_period = ''\n",
    "    if month is not None:\n",
    "        time_period += f\"{month}/\"\n",
    "\n",
    "    time_period += f\"{datetime.now().year}\" if year is None else f\"{year}\" if year != 0 else \"\"\n",
    "\n",
    "    date_container = soup.find(\"div\", {\"id\": \"container_date\"})\n",
    "    logging.info(f\"Getting links for {time_period}\")\n",
    "    links = []\n",
    "    for a in date_container.find_all(\"a\"):\n",
    "        if f\"{time_period}\" in a['href']:\n",
    "            links.append(a['href'])\n",
    "    logging.info(f\"Found {len(links)} links for {time_period}\\n\" + '\\n'.join(links))\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_decisions_metadata(soup):\n",
    "    \"\"\"\n",
    "    Get metadata of the decisions in the current page.\n",
    "    :param soup:    BeautifulSoup object\n",
    "    :return:        List of metadata\n",
    "    \"\"\"\n",
    "    decisions = soup.find(\"div\", {\"id\": \"container_title\"}).find(\"ul\").find_all(\"li\")\n",
    "    metadata = []\n",
    "\n",
    "    logging.info(f\"Found {len(decisions)} decisions\")\n",
    "\n",
    "    for decision in decisions:\n",
    "        raw_metadata = decision.find(\"a\")\n",
    "\n",
    "        # Get the URL\n",
    "        current_metadata = {\"url\": raw_metadata[\"href\"]}\n",
    "        logging.debug(f\"URL: {current_metadata['url']}\")\n",
    "\n",
    "        # Get the title\n",
    "        current_metadata[\"title\"] = raw_metadata.find(\"strong\").text\n",
    "        logging.debug(f\"Title: {current_metadata['title']}\")\n",
    "\n",
    "        # Get parties\n",
    "        parties = raw_metadata.find(\"small\")\n",
    "        current_metadata[\"subtitle\"] = parties.text.strip()\n",
    "\n",
    "        # Get the date\n",
    "        date = parties.nextSibling.strip()\n",
    "        # Create date object\n",
    "        current_metadata[\"date\"] = datetime.strptime(date, \"%B %d, %Y\").date().isoformat()\n",
    "        logging.debug(f\"Date: {current_metadata['date']}\")\n",
    "\n",
    "        # Log divider\n",
    "        logging.debug('-' * 50)\n",
    "\n",
    "        # Append to list\n",
    "        metadata.append(current_metadata)\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e89fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
